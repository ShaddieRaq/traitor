# Bot Migration Strategy - Phase 6.3 Implementation Plan

## ðŸŽ¯ OBJECTIVE
Migrate all 25 bots from individual Coinbase API calls to shared cache reads, eliminating rate limiting errors while maintaining system functionality.

## ðŸ“Š MIGRATION IMPACT ANALYSIS

### Current Bot Evaluation Process (PROBLEMATIC)
```python
# Current pattern in bot_evaluator.py - CAUSES RATE LIMITS
for bot in active_bots:  # 25 bots
    ticker = coinbase_service.get_product_ticker(bot.pair)        # API CALL 1
    historical = coinbase_service.get_historical_data(bot.pair)   # API CALL 2  
    accounts = coinbase_service.get_accounts()                    # API CALL 3
    balance_check = coinbase_service.validate_trade_balance()     # API CALL 4
    # Result: 25 Ã— 4 = 100 API calls every 5 minutes
```

### Target Bot Evaluation Process (FIXED)
```python
# New pattern with shared cache - ZERO RATE LIMITS
async def evaluate_bots_coordinated():
    # Single coordinated data fetch for ALL bots
    all_data = await fetch_all_market_data_coordinated()
    
    for bot in active_bots:  # 25 bots
        ticker = all_data['tickers'][bot.pair]        # CACHE READ
        historical = all_data['historical'][bot.pair] # CACHE READ
        accounts = all_data['accounts']               # CACHE READ
        # Result: 0 individual API calls, all data from cache
```

## ðŸ”„ MIGRATION PHASES

### Phase 6.3.1: Update BotSignalEvaluator (Week 1)
**Goal**: Migrate core bot evaluation from direct API calls to shared cache

#### Current File: `/backend/app/services/bot_evaluator.py`
**Changes Required**:

1. **Add Async Support**
```python
# BEFORE (Synchronous with direct API calls)
class BotSignalEvaluator:
    def evaluate_bot(self, bot: Bot) -> dict:
        ticker = coinbase_service.get_product_ticker(bot.pair)  # DIRECT API
        
# AFTER (Asynchronous with coordinated calls)
class BotSignalEvaluator:
    async def evaluate_bot(self, bot: Bot) -> dict:
        from ..services.api_coordinator import coordinated_get_ticker
        ticker = await coordinated_get_ticker(bot.pair)  # COORDINATED
```

2. **Batch Data Fetching**
```python
# NEW METHOD: Fetch all data for all bots in one coordinated batch
async def fetch_coordinated_market_data(self, bot_pairs: List[str]) -> Dict[str, Any]:
    """Fetch all market data needed for bot evaluation in coordinated manner."""
    
    # Submit all requests to coordinator (will use cache if available)
    ticker_requests = []
    historical_requests = []
    
    for pair in bot_pairs:
        ticker_requests.append(coordinated_get_ticker(pair))
        historical_requests.append(coordinated_get_historical(pair))
    
    # Get accounts once for all bots
    accounts = await coordinated_get_accounts()
    
    # Execute all requests (coordinator will batch/cache appropriately)
    tickers = await asyncio.gather(*ticker_requests)
    historical_data = await asyncio.gather(*historical_requests)
    
    return {
        'tickers': dict(zip(bot_pairs, tickers)),
        'historical': dict(zip(bot_pairs, historical_data)),
        'accounts': accounts
    }
```

3. **Update evaluate_all_bots Method**
```python
# BEFORE (Individual API calls per bot)
def evaluate_all_bots(self) -> List[dict]:
    results = []
    for bot in active_bots:
        result = self.evaluate_bot(bot)  # Each bot makes 3-4 API calls
        results.append(result)
    return results

# AFTER (Coordinated batch processing)
async def evaluate_all_bots(self) -> List[dict]:
    # Get all bot pairs
    bot_pairs = [bot.pair for bot in self.active_bots]
    
    # Fetch all market data in coordinated manner
    market_data = await self.fetch_coordinated_market_data(bot_pairs)
    
    # Evaluate all bots using cached data
    results = []
    for bot in self.active_bots:
        result = await self.evaluate_bot_with_data(bot, market_data)
        results.append(result)
    
    return results
```

### Phase 6.3.2: Update Celery Trading Tasks (Week 2)
**Goal**: Migrate Celery tasks from direct API calls to coordinated system

#### Current File: `/backend/app/tasks/trading_tasks.py`
**Changes Required**:

1. **Update fast_trading_evaluation Task**
```python
# BEFORE (Direct API calls in Celery task)
@celery_app.task(name="app.tasks.trading_tasks.fast_trading_evaluation")
def fast_trading_evaluation():
    evaluator = BotSignalEvaluator(db)
    results = evaluator.evaluate_all_bots()  # Makes 100+ API calls

# AFTER (Coordinated calls in Celery task)
@celery_app.task(name="app.tasks.trading_tasks.fast_trading_evaluation")
def fast_trading_evaluation():
    # Run async evaluation in sync Celery context
    import asyncio
    
    async def async_evaluation():
        evaluator = BotSignalEvaluator(db)
        return await evaluator.evaluate_all_bots()  # Uses shared cache
    
    results = asyncio.run(async_evaluation())
```

2. **Start API Coordinator in Celery**
```python
# Add to celery_app.py or trading_tasks.py
from ..services.api_coordinator import api_coordinator

# Ensure coordinator is running when Celery starts
@celery_app.on_after_configure.connect
def setup_api_coordinator(sender, **kwargs):
    api_coordinator.start()
    logger.info("ðŸš¦ API coordinator started for Celery workers")
```

### Phase 6.3.3: Update Market API Endpoints (Week 3)
**Goal**: Migrate FastAPI endpoints from direct API calls to shared cache

#### Current File: `/backend/app/api/market.py`
**Changes Required**:

1. **Update Ticker Endpoint**
```python
# BEFORE (Direct API call)
@router.get("/ticker/{product_id}")
def get_ticker(product_id: str):
    ticker = coinbase_service.get_product_ticker(product_id)  # DIRECT API

# AFTER (Coordinated call)
@router.get("/ticker/{product_id}")
async def get_ticker(product_id: str):
    from ..services.api_coordinator import coordinated_get_ticker
    ticker = await coordinated_get_ticker(product_id)  # COORDINATED
    if not ticker:
        raise HTTPException(status_code=404, detail="Ticker unavailable")
    return ticker
```

2. **Update Accounts Endpoint**
```python
# BEFORE (Direct API call)
@router.get("/accounts")
def get_accounts():
    accounts = coinbase_service.get_accounts()  # DIRECT API

# AFTER (Coordinated call) 
@router.get("/accounts")
async def get_accounts():
    from ..services.api_coordinator import coordinated_get_accounts
    accounts = await coordinated_get_accounts()  # COORDINATED
    return accounts or []
```

### Phase 6.3.4: Update Data Tasks (Week 4)
**Goal**: Migrate background data fetching to coordinated system

#### Current File: `/backend/app/tasks/data_tasks.py`
**Changes Required**:

1. **Update Market Data Fetch Task**
```python
# BEFORE (Direct API calls)
@celery_app.task
def fetch_market_data_task(product_ids: list = None):
    for product_id in product_ids:
        df = coinbase_service.get_historical_data(product_id)  # DIRECT API

# AFTER (Coordinated calls)
@celery_app.task  
def fetch_market_data_task(product_ids: list = None):
    import asyncio
    
    async def fetch_coordinated():
        from ..services.api_coordinator import coordinated_get_historical
        
        tasks = []
        for product_id in product_ids:
            task = coordinated_get_historical(product_id)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return dict(zip(product_ids, results))
    
    data = asyncio.run(fetch_coordinated())
```

## ðŸ§ª TESTING STRATEGY

### Phase 6.3.T1: Validation Testing
1. **Cache Performance Test**
```python
# Test cache hit rates with new system
async def test_cache_performance():
    # Submit 100 requests for same data
    results = []
    for i in range(100):
        result = await coordinated_get_ticker("BTC-USD")
        results.append(result)
    
    # Should have >95% cache hits after first request
    assert len(set(str(r) for r in results)) == 1  # All same data
```

2. **Rate Limiting Prevention Test**
```python
# Test that we never exceed rate limits
async def test_rate_limiting_prevention():
    start_time = time.time()
    
    # Submit 1000 requests rapidly
    tasks = []
    for i in range(1000):
        task = coordinated_get_ticker("BTC-USD")
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    
    # Should complete without any 429 errors
    assert all(r is not None for r in results)
    assert api_coordinator.get_stats()['rate_limit_errors'] == 0
```

### Phase 6.3.T2: Performance Comparison
```python
# Compare old vs new system performance
async def performance_comparison():
    # Old system simulation (individual calls)
    start_old = time.time()
    old_results = []
    for i in range(25):  # 25 bots
        # Simulate 4 API calls per bot
        for j in range(4):
            time.sleep(0.1)  # Simulate API delay
            old_results.append(f"result_{i}_{j}")
    old_time = time.time() - start_old
    
    # New system (coordinated)
    start_new = time.time()
    new_results = []
    for i in range(25):
        result = await coordinated_get_ticker(f"PAIR-{i}")
        new_results.append(result)
    new_time = time.time() - start_new
    
    # New system should be significantly faster
    assert new_time < old_time * 0.1  # 90% faster
```

## ðŸ“ˆ ROLLBACK STRATEGY

### Safe Rollback Plan
1. **Feature Flag Implementation**
```python
# Add feature flag to gradually migrate
USE_COORDINATED_API_CALLS = os.getenv('USE_COORDINATED_API_CALLS', 'false').lower() == 'true'

async def get_ticker_safe(product_id: str):
    if USE_COORDINATED_API_CALLS:
        return await coordinated_get_ticker(product_id)
    else:
        return coinbase_service.get_product_ticker(product_id)  # Fallback
```

2. **Gradual Migration**
```python
# Migrate bots in batches
COORDINATED_BOT_IDS = os.getenv('COORDINATED_BOT_IDS', '').split(',')

async def evaluate_bot_safe(bot: Bot):
    if str(bot.id) in COORDINATED_BOT_IDS:
        return await evaluate_bot_coordinated(bot)  # New system
    else:
        return evaluate_bot_legacy(bot)  # Old system
```

## ðŸŽ¯ SUCCESS METRICS

### Key Performance Indicators
1. **Rate Limiting Elimination**: 0 Ã— 429 errors (down from 100+)
2. **API Call Reduction**: <50 calls/hour (down from 1,200+)
3. **Cache Hit Rate**: >95% (up from 78%)
4. **Bot Evaluation Time**: <30 seconds for all 25 bots
5. **System Reliability**: 99.9% uptime

### Monitoring Dashboard
```python
# Real-time monitoring endpoints
@router.get("/migration/stats")
async def get_migration_stats():
    return {
        "api_coordinator": api_coordinator.get_stats(),
        "cache_manager": await shared_cache_manager.get_cache_stats(),
        "migration_progress": {
            "bots_migrated": len(COORDINATED_BOT_IDS),
            "total_bots": 25,
            "migration_percentage": (len(COORDINATED_BOT_IDS) / 25) * 100
        }
    }
```

## ðŸš¨ CRITICAL CHECKPOINTS

### Before Each Phase
1. **Backup System State**: Full database backup
2. **Rate Limit Check**: Confirm current error count
3. **Performance Baseline**: Record current response times
4. **Cache Status**: Verify Redis connectivity

### After Each Phase  
1. **Rate Limit Verification**: Zero 429 errors for 24 hours
2. **Functionality Test**: All bots still operating correctly
3. **Performance Check**: Response times maintained or improved
4. **Cache Metrics**: Hit rates and memory usage within limits

---

**Status**: Phase 6.3 Migration Strategy Complete âœ…  
**Next**: Begin Phase 6.3.1 - BotSignalEvaluator Migration  
**Goal**: Systematic migration with zero downtime and zero rate limiting